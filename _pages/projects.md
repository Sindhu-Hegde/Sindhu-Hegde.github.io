---
layout: page
title: Projects
permalink: /projects/
description: A list of exciting projects that I have been a part of! Will be updated as and when possible.
nav: true
display_categories: [work, fun]
horizontal: true
---

<h3 style="text-align:right; color:#D3D3D3;">publications</h3> 
<hr>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

  <tr>
    <td style="width:25%;vertical-align:middle">
      <div class="one">
        <img src='/assets/img/acm_videosr_banner.png' width="300" height="140">
      </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://dl.acm.org/doi/10.1145/3503161.3548080">
        <papertitle><b>Extreme-scale Talking-Face Video Upsampling with Audio-Visual Priors</b></papertitle>
      </a>
      <br>
      <strong>Sindhu B Hegde*</strong>,
      Rudrabha Mukhopadhyay*,  
      Vinay P Namboodiri,
      C. V. Jawahar
      <br>
      <em>ACM-MM</em>, 2022 <span style="color:red"></span>
      <br>
      <a href="https://arxiv.org/pdf/2208.08118.pdf">arXiv</a> /
      <a href="https://github.com/Sindhu-Hegde/video-super-resolver">code</a> /
      <a href="http://cvit.iiit.ac.in/research/projects/cvit-projects/talking-face-video-upsampling">project page</a> 
      <p></p>
      <p>Super-resolve extremely low-resolution videos (eg., 8x8 pixels) to obtain realistic, high-resolution outputs (256x256 pixels). Upsampled the videos at a scale-factor of 32x for the first time!.</p>
    </td>
  </tr>

  <tr>
    <td style="width:25%;vertical-align:middle">
      <div class="one">
        <img src='/assets/img/acm_l2s_banner.png' width="320" height="120">
      </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://dl.acm.org/doi/10.1145/3503161.3548081">
        <papertitle><b>Lip-to-Speech Synthesis for Arbitrary Speakers in the Wild</b></papertitle>
      </a>
      <br>
      <strong>Sindhu B Hegde*</strong>,
      K R Prajwal*,
      Rudrabha Mukhopadhyay*,  
      Vinay P Namboodiri,
      C. V. Jawahar
      <br>
      <em>ACM-MM</em>, 2022 <span style="color:red"></span>
      <br>
      <a href="https://arxiv.org/pdf/2209.00642.pdf">arXiv</a> /
      <a href="https://github.com/Sindhu-Hegde/lip2speech">code</a> /
      <a href="http://cvit.iiit.ac.in/research/projects/cvit-projects/lip-to-speech-synthesis">project page</a> 
      <p></p>
      <p>Generates speech for silent talking face videos for any speaker in-the-wild! One of the first models to work for arbitrary speakers, with no explicit costraints in the domain or vocabulary.</p>
    </td>
  </tr>

  <tr>
    <td style="width:25%;vertical-align:middle">
      <div class="one">
        <img src='/assets/img/bmvc21_banner.png' width="320" height="100">
      </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://www.bmvc2021-virtualconference.com/assets/papers/0930.pdf">
        <papertitle><b>Audio-Visual Speech Super-Resolution</b></papertitle>
      </a>
      <br>
      Rudrabha Mukhopadhyay*, 
      <strong>Sindhu B Hegde*</strong>, 
      Vinay P Namboodiri,
      C. V. Jawahar
      <br>
      <em>BMVC</em>, 2021 <span style="color:red"><b>(Oral Presentation)</b></span>
      <br>
      <a href="https://www.bmvc2021-virtualconference.com/assets/papers/0930.pdf">pdf</a> /
      <a href="https://www.bmvc2021-virtualconference.com/conference/papers/paper_0930.html">presentation</a> /
      <a href="http://cvit.iiit.ac.in/research/projects/cvit-projects/audio-visual-speech-super-resolution">project page</a> 
      <p></p>
      <p>An audio-visual model to super-reolve very low-resolution speech signals (e.g., 1kHz) & generate high-quality speech (16kHz). Works even if the real-visual stream is unavailable/corrupted using the proposed pseudo-visual approach!</p>
    </td>
  </tr>

  <tr>
    <td style="width:25%;vertical-align:middle">
      <div class="one">
        <img src='/assets/img/wacv21_banner.gif' width="320" height="200" style="vertical-align:middle">
      </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://openaccess.thecvf.com/content/WACV2021/papers/Hegde_Visual_Speech_Enhancement_Without_a_Real_Visual_Stream_WACV_2021_paper.pdf">
        <papertitle><b>Visual Speech Enhancement Without A Real Visual Stream</b></papertitle>
      </a>
      <br>
      <strong>Sindhu B Hegde*</strong>, 
      K R Prajwal*,
      Rudrabha Mukhopadhyay*, 
      Vinay P Namboodiri,
      C. V. Jawahar
      <br>
      <em>WACV</em>, 2021
      <br>
      <a href="https://arxiv.org/abs/2012.10852">arXiv</a> /
      <a href="https://www.youtube.com/watch?v=y_oP9t7WEn4">demo video</a> /
      <a href="https://www.youtube.com/watch?v=Dqx6d-XdQCI">presentation</a> /
      <a href="https://github.com/Sindhu-Hegde/pseudo-visual-speech-denoising">code</a> /
      <a href="http://cvit.iiit.ac.in/research/projects/cvit-projects/visual-speech-enhancement-without-a-real-visual-stream">project page</a> 
      <p></p>
      <p>A new paradigm for speech enhancement that works effectively in unconstrained, high-noise, real-world environments. A hybrid approach to hallucinate the visual stream using only the noisy speech as input.</p>
    </td>
  </tr>

  <tr>
    <td style="width:25%;vertical-align:middle">
      <div class="one">
        <img src='/assets/img/interspeech21_banner.png' width="318" height="180" style="vertical-align:middle">
      </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://arxiv.org/pdf/2106.12790.pdf">
        <papertitle><b>Towards Automatic Speech to Sign Language Generation</b></papertitle>
      </a>
      <br>
      Parul Kapoor,
      Rudrabha Mukhopadhyay, 
      <strong>Sindhu B Hegde</strong>, 
      Vinay P Namboodiri,
      C. V. Jawahar
      <br>
      <em>INTERSPEECH</em>, 2021
      <br>
      <a href="https://arxiv.org/abs/2012.10852">arXiv</a> /
      <a href="http://cvit.iiit.ac.in/images/Projects/speech2sl/INTERSPEECH_long_video.mp4">demo video</a> /
      <a href="https://github.com/kapoorparul/Towards-Automatic-Speech-to-SL">code</a> /
      <a href="http://cvit.iiit.ac.in/research/projects/cvit-projects/towards-speech-to-sign-language-generation">project page</a> 
      <p></p>
      <p>Generated continuous sign-language videos solely from speech segments for the first time. Also curated and released the first Indian Sign Language (ISL) dataset comprising speech annotations, transcripts & sign-language videos.</p>
    </td>
  </tr>

  <tr>
    <td style="width:25%;vertical-align:middle">
      <div class="one">
        <img src='/assets/img/icprdemo_banner.png' width="318" height="160" style="vertical-align:middle">
      </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://arxiv.org/pdf/2106.12790.pdf">
        <papertitle><b>The Interplay of Speech and Lip Movements </b></papertitle>
      </a>
      <br>
      Rudrabha Mukhopadhyay*,
      K R Prajwal*, 
      <strong>Sindhu B Hegde*</strong>, 
      Vinay P Namboodiri,
      C. V. Jawahar
      <br>
      <em>ICPR Demonstrations</em>, 2020
      <br>
      <a href="https://www.youtube.com/watch?v=ydj4Ach3d8I">demo video</a> /
      <a href="https://www.micc.unifi.it/icpr2020/wp-content/uploads/demos/s3.2-paper.pdf">writeup</a>  
      <p></p>
      <p>Extensively explore the correlation between vision and speech modalities, specifically the speech and lip movements.</p>
    </td>
  </tr>

</tbody></table>


<h3 style="text-align:right; color:#D3D3D3;">implementations</h3> 
<hr>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

  <tr>
    <td style="width:25%;vertical-align:middle">
      <div class="one">
        <img src='/assets/img/spsep_banner.png' width="320" height="140">
      </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://github.com/Sindhu-Hegde/speaker-separation">
        <papertitle><b>Audio-Visual Speaker Separation</b></papertitle>
      </a>
      <br>
      <a href="https://github.com/Sindhu-Hegde/speaker-separation">code</a> /
      <a href="https://colab.research.google.com/github/Sindhu-Hegde/speaker-separation/blob/main/speaker_separation_inference.ipynb">colab</a> /
      <a href="https://docs.google.com/presentation/d/1dqzbBOLL7-bHdJ0rWB4Mo-tEaHn_9p_qkw8yIbl73jA/edit?usp=sharing">presentation</a> 
      <p></p>
      <p>Separate the two speakers talking simultaneously in a cocktail-party like situation. An audio-visual model to enhance & isolate the speech of the target speaker.</p>
      <b>Papers referred:</b><br>
      (1) <a href="https://arxiv.org/pdf/1804.04121.pdf">The Conversation: Deep Audio-Visual Speech Enhancement</a><br>
      (2) <a href="https://dl.acm.org/doi/pdf/10.1145/3197517.3201357">Looking to Listen at the Cocktail Party: A Speaker-Independent Audio-Visual Model for Speech Separation</a>
    </td>
  </tr>

  <tr>
    <td style="width:25%;vertical-align:middle">
      <div class="one">
        <img src='/assets/img/yst_banner.png' width="320" height="60">
      </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://github.com/Sindhu-Hegde/you_said_that">
        <papertitle><b>Speech-driven Lip Synthesis</b></papertitle>
      </a>
      <br>
      <a href="https://github.com/Sindhu-Hegde/you_said_that">code</a>
      <p></p>
      <p>Generate a talking-face video from the still image of the target identity & the corresponding speech segment. Works for unseen faces & audios!</p>
      <b>Papers referred:</b><br>
      (1) <a href="https://www.robots.ox.ac.uk/~vgg/publications/2017/Chung17b/chung17b.pdf">You said that?</a><br>
      (2) <a href="https://arxiv.org/pdf/2003.00418.pdf">Towards Automatic Face-to-Face Translation</a>
    </td>
  </tr>

</tbody></table>
