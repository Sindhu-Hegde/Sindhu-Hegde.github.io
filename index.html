<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Sindhu B. Hegde</title>
    <meta name="author" content="Sindhu B. Hegde" />
    <meta name="description" content="Webgape - Sindhu B Hegde
" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/icon.jpg"/>
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://sindhu-hegde.github.io/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/assets/pdf/Resume_SindhuHegde.pdf">CV</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">Projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>

              <!-- Toogle theme mode -->
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <div class="post">

  <!-- <header class="post-header">
    <h1 class="post-title">
     <span class="font-weight-bold">Sindhu</span> B.  Hegde
    </h1>
     <p class="desc"></p>
  </header> -->

  <div class="post">
        <header class="post-header">
          <h1 class="post-title">
           <span class="font-weight-bold">Sindhu</span> B. Hegde
          </h1>
          <p class="desc"><b>PhD Student</b>, <a href="https://www.ox.ac.uk" target="_blank" rel="noopener noreferrer">University of Oxford</a></p>
        </header>

  <article>
    
    <div class="profile float-right">
      
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/prof_pic.jpg">
      
      
        <div class="address">
          
        </div>
      
    
    </div>
    

    <div class="clearfix">
      <p>Hi! I am a second year PhD student in the <a href="https://www.robots.ox.ac.uk/~vgg/" target="_blank" rel="noopener noreferrer">Visual Geometry Group (VGG)</a> at the University of Oxford, supervised by <a href="https://scholar.google.co.uk/citations?user=UZ5wscMAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Prof. Andrew Zisserman</a>. My research is in Computer Vision, particularly in multimodal learning, video understanding and self-supervised learning.
<!-- with an immense interest and enthusiasm in the areas of Computer Vision & AI.  --></p>

<p>Prior to joining Oxford, I worked as a Lead Data Scientist @ <a href="https://www.verisk.com" target="_blank" rel="noopener noreferrer">Verisk Analytics</a>. Before that, I pursued Masters’ by Research (MS) at <a href="http://cvit.iiit.ac.in" target="_blank" rel="noopener noreferrer">Centre for Visual Information Technology (CVIT), IIIT Hyderabad</a> supervised by <a href="https://faculty.iiit.ac.in/~jawahar/index.html" target="_blank" rel="noopener noreferrer">Prof. C V Jawahar</a> (IIIT-H) and <a href="https://vinaypn.github.io" target="_blank" rel="noopener noreferrer">Prof. Vinay Namboodiri</a> (University of Bath, UK). My Masters’ research focused on exploiting the redundancies in vision and speech modalities for cross-modal generation.</p>

<p><strong>Research interests:</strong> Computer Vision, Machine Learning, Deep Learning, Video Understanding, Multi-modal Learning: Vision + Speech/Language <br><br></p>

    </div>

    
      <div class="news">
  <h3>News <a href="/news/highlights/"> [Archive]  </a> </h3>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
      
      
      
      
        <tr>
          <th style="width: 10%" scope="row">Sep 2023</th>
          <td>
            
              Our paper on <a href="https://www.robots.ox.ac.uk/vgg/research/gestsync/" target="_blank" rel="noopener noreferrer">GestSync: Determining who is speaking without a talking head</a> accepted to BMVC 2023 (<span style="color:red">ORAL</span>)

            
          </td>
        </tr>
      
      
      
        <tr>
          <th style="width: 10%" scope="row">Jul 2023</th>
          <td>
            
              Participated in the <a href="https://iplab.dmi.unict.it/icvss2023/Home" target="_blank" rel="noopener noreferrer">International Computer Vision Summer School (ICVSS)</a>) at Sicily, Italy. Had an eincredible experience of learning from some of the most distinguished computer vision experts!

            
          </td>
        </tr>
      
      
      
        <tr>
          <th style="width: 10%" scope="row">Oct 2022</th>
          <td>
            
              Joined the Visual Geometry Group (VGG) at the <a href="https://www.ox.ac.uk" target="_blank" rel="noopener noreferrer">University of Oxford</a> as a <strong>PhD student</strong> with <a href="https://scholar.google.com/citations?user=UZ5wscMAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Prof. Andrew Zisserman</a>

            
          </td>
        </tr>
      
      
      
        <tr>
          <th style="width: 10%" scope="row">Jul 2022</th>
          <td>
            
              2 papers accepted to <a href="https://2022.acmmm.org" target="_blank" rel="noopener noreferrer">ACM-MM 2022</a>
<br> 1] <a href="https://arxiv.org/pdf/2208.08118.pdf" target="_blank" rel="noopener noreferrer">Talking-Face Video Upsampling</a>  2] <a href="https://arxiv.org/pdf/2209.00642.pdf" target="_blank" rel="noopener noreferrer">Lip-to-Speech Synthesis</a>

            
          </td>
        </tr>
      
      
      
        <tr>
          <th style="width: 10%" scope="row">May 2022</th>
          <td>
            
              Successfully defended MS thesis <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20"> 
<br> <strong>Thesis:</strong> <a href="https://web2py.iiit.ac.in/research_centres/publications/view_publication/mastersthesis/1110" target="_blank" rel="noopener noreferrer">Exploiting Cross-Modal Redundancy for Audio-Visual Generation</a>

            
          </td>
        </tr>
      
      
      </table>
    </div>
  
</div>


    

    
      <div class="publications">
  <h3>Recent papers <a href="/publications/"> [Full list] </a> </h3>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">BMVC</abbr></div>

        <!-- Entry bib key -->
        <div id="GestSync_2023_BMVC" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">GestSync: Determining who is speaking without a talking head</div>
          <!-- Author -->
          <div class="author">
                  <em>Hegde, Sindhu</em>, and Zisserman, Andrew
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In British Machine Vision Conference (BMVC)</em> 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="/assets/pdf/" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a href="https://github.com/Sindhu-Hegde/gestsync" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="https://www.robots.ox.ac.uk/vgg/research/gestsync/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>In this paper we introduce a new synchronisation task, Gesture-Sync: determining if a person’s gestures are correlated with their speech or not. In comparison to Lip-Sync, Gesture-Sync is far more challenging as there is a far looser relationship between the voice and body movement than there is between voice and lip motion. We introduce a dual-encoder model for this task, and compare a number of input representations including RGB frames, keypoint images, and keypoint vectors, assessing their performance and advantages. We show that the model can be trained using self-supervised learning alone, and evaluate its performance on the LRS3 dataset. Finally, we demonstrate applications of Gesture-Sync for audio-visual synchronisation, and in determining who is the speaker in a crowd, without seeing their faces.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ACM-MM</abbr></div>

        <!-- Entry bib key -->
        <div id="ExtremeSR_2022_ACMMM" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Extreme-scale Talking-Face Video Upsampling with Audio-Visual Priors</div>
          <!-- Author -->
          <div class="author">
                  <em>Hegde, Sindhu</em>, Mukhopadhyay, Rudrabha, Namboodiri, Vinay P, and Jawahar, CV
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the 30th ACM International Conference on Multimedia (MM’22)</em> 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://arxiv.org/pdf/2208.08118.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/Sindhu-Hegde/video-super-resolver" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="http://cvit.iiit.ac.in/research/projects/cvit-projects/talking-face-video-upsampling" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>In this paper, we explore an interesting question of what can be obtained from an 8×8 pixel video sequence. Surprisingly, it turns out to be quite a lot. We show that when we process this 8x8 video with the right set of audio and image priors, we can obtain a full-length, 256x256 video. We achieve this 32x scaling of an extremely low-resolution input using our novel audio-visual upsampling network. The audio prior helps to recover the elemental facial details and precise lip shapes, and a single high-resolution target identity image prior provides us with rich appearance details. Our approach is an end-to-end multi-stage framework. The first stage produces a coarse intermediate output video that can be then used to animate the single target identity image and generate realistic, accurate and high-quality outputs. Our approach is simple and performs exceedingly well (an 8× improvement in FID score) compared to previous super-resolution methods. We also extend our model to talking-face video compression, and show that we obtain a 3.5x improvement in terms of bits/pixel over the previous state-of-the-art. The results from our network are thoroughly analyzed through extensive ablation and comparative analysis and demonstration videos (in the paper and supplementary material).</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ACM-MM</abbr></div>

        <!-- Entry bib key -->
        <div id="L2S_2022_ACMMM" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Lip-to-Speech Synthesis for Arbitrary Speakers in the Wild</div>
          <!-- Author -->
          <div class="author">
                  <em>Hegde, Sindhu</em>, Prajwal, KR, Mukhopadhyay, Rudrabha, Namboodiri, Vinay P, and Jawahar, CV
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the 30th ACM International Conference on Multimedia (MM’22)</em> 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://arxiv.org/pdf/2209.00642.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/Sindhu-Hegde/lip2speech" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
            <a href="http://cvit.iiit.ac.in/research/projects/cvit-projects/lip-to-speech-synthesis" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>In this work, we address the problem of generating speech from silent lip videos for any speaker in the wild. In stark contrast to previous works in lip-to-speech synthesis, our work (i) is not restricted to a fixed number of speakers, (ii) does not explicitly impose constraints on the domain or the vocabulary and (iii) deals with videos that are recorded in the wild as opposed to within laboratory settings. The task presents a host of challenges with the key one being that many features of the desired target speech like voice, pitch and linguistic content cannot be entirely inferred from the silent face video. In order to handle these stochastic variations, we propose a new VAE-GAN architecture that learns to associate the lip and speech sequences amidst the variations. With the help of multiple powerful discriminators that guide the training process, our generator learns to synthesize speech sequences in any voice for the lip movements of any person. Extensive experiments on multiple datasets show that we outperform all baseline methods by a large margin. Further, our network can be fine-tuned on videos of specific identities to achieve a performance comparable to single-speaker models that are trained on 4x more data. We also conduct numerous ablation studies to analyze the effect of different modules of our architecture. A demo video in supplementary material demonstrates several qualitative results and comparisons.</p>
          </div>
        </div>
      </div>
</li>
</ol>
</div>

    

    
    <div class="social">
            <div class="contact-icons">
            <a href="mailto:%73%69%6E%64%68%75@%72%6F%62%6F%74%73.%6F%78.%61%63.%75%6B" title="email"><i class="fas fa-envelope"></i></a>
            <a href="https://scholar.google.com/citations?user=cD8J2-kAAAAJ&amp;hl=en" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>
            <a href="https://github.com/Sindhu-Hegde" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a>
            <a href="https://www.linkedin.com/in/sindhu-b-hegde" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a>
            
<!-- -->
            </div>

            <div class="contact-note">
              The best way to reach out to me is the above email.

            </div>
            
          </div>

    
  </article>

</div>

    </div>

    <!-- Footer -->


<footer class="fixed-bottom">
  <div class="container mt-0">
    © Copyright 2023 Sindhu B. Hegde.
    <c> <br> <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme on <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. </c>

    
    Last updated: October 04, 2023.
    
  </div>
</footer>



    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </div></body>
</html>

